{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aba25d",
   "metadata": {},
   "source": [
    "# Hello, MNIST!\n",
    "\n",
    "a quick throwaway comparison of a basic CNN to more classical techniques on the meme-iest image recognition dataset, MNIST!  \n",
    "\n",
    "(I haven't used NNets too much in the past few years, wanted to refresh my memory.)\n",
    " \n",
    "the short version is - classical approaches perform shockingly well here, achieving 95-98% accuracy. NN get to 99%+ with very little effort (single convolution+maxpool will get you there).  \n",
    "\n",
    "of course, classical approaches don't work very well on many other image classifcation problems, but interesting to see just _how_ easy MNIST seems to be.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c10b49",
   "metadata": {},
   "source": [
    "## classical\n",
    "\n",
    "pca / ica / umap\n",
    "+ svm / gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd08b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b683dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/2022-02/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA, NMF, FastICA \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from umap import UMAP\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N_COMPONENTS = 50\n",
    "\n",
    "std_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "pca = PCA(n_components=N_COMPONENTS, random_state=42)\n",
    "nmf = NMF(n_components=N_COMPONENTS, random_state=42, max_iter=500, init='nndsvda')\n",
    "\n",
    "svc = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    probability=False,\n",
    ")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=.5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "lgbc = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=.05,\n",
    "    subsample=.9,\n",
    "    random_state=42,\n",
    ")\n",
    "sklgbc = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=1_000,\n",
    "    max_leaf_nodes=31,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=10,\n",
    "    l2_regularization=1.0,\n",
    "    max_bins=255,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pca_pipe = make_pipeline(std_scaler, pca, sklgbc)\n",
    "nmf_pipe = make_pipeline(nmf, sklgbc)\n",
    "lgb_pipe = make_pipeline(nmf, lgbc)\n",
    "rf_pipe = make_pipeline(nmf, rf)\n",
    "svm_pipe = make_pipeline(nmf, svc)\n",
    "# note: have to reduce before umap or we'll get errors. \n",
    "# defaulting to nmf b/c all pos. image data\n",
    "# edit: struggles past n=1k or so, even with preprocessing. omit.  \n",
    "#umap_pipe = make_pipeline(smol_nmf, std_scaler, umap, gb)\n",
    "\n",
    "# Get dims + reshape from 2d per sample to standard instances x features.\n",
    "n, x_dim, y_dim, _ = x_train.shape\n",
    "x_train_flat = x_train.reshape((n, x_dim * y_dim))\n",
    "\n",
    "n, x_dim, y_dim, _ = x_test.shape\n",
    "x_test_flat = x_test.reshape((n, x_dim * y_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4b6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0.9632 \n",
      " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('pca', PCA(n_components=50, random_state=42)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(l2_regularization=1.0,\n",
      "                                                learning_rate=0.05, max_depth=6,\n",
      "                                                max_iter=1000,\n",
      "                                                min_samples_leaf=10,\n",
      "                                                random_state=42))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/2022-02/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0.9786 \n",
      " Pipeline(steps=[('nmf',\n",
      "                 NMF(init='nndsvda', max_iter=500, n_components=50,\n",
      "                     random_state=42)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(l2_regularization=1.0,\n",
      "                                                learning_rate=0.05, max_depth=6,\n",
      "                                                max_iter=1000,\n",
      "                                                min_samples_leaf=10,\n",
      "                                                random_state=42))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/2022-02/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0.9801 \n",
      " Pipeline(steps=[('nmf',\n",
      "                 NMF(init='nndsvda', max_iter=500, n_components=50,\n",
      "                     random_state=42)),\n",
      "                ('lgbmclassifier',\n",
      "                 LGBMClassifier(learning_rate=0.05, n_estimators=500,\n",
      "                                random_state=42, subsample=0.9))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/2022-02/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0.9138 \n",
      " Pipeline(steps=[('nmf',\n",
      "                 NMF(init='nndsvda', max_iter=500, n_components=50,\n",
      "                     random_state=42)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(max_depth=8, max_features=0.5,\n",
      "                                        min_samples_leaf=5,\n",
      "                                        min_samples_split=10, n_jobs=-1,\n",
      "                                        random_state=42))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/2022-02/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0.9548 \n",
      " Pipeline(steps=[('nmf',\n",
      "                 NMF(init='nndsvda', max_iter=500, n_components=50,\n",
      "                     random_state=42)),\n",
      "                ('svc', SVC())])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in [pca_pipe, nmf_pipe, lgb_pipe, rf_pipe, svm_pipe]:\n",
    "    model.fit(x_train_flat, y_train)\n",
    "\n",
    "    #pred_probs = model.predict_proba(x_test_flat)\n",
    "    pred_classes = model.predict(x_test_flat)\n",
    "\n",
    "    # AUC as we'd often use in traditional balanced-ish tabular problems.\n",
    "    #auc = roc_auc_score(\n",
    "    #    y_test, pred_probs, multi_class=\"ovr\", average=\"weighted\",\n",
    "    #)\n",
    "    \n",
    "    # Also accuracy, since MNIST is often judged this way, and it's an imbalanced, multiclass problem.\n",
    "    acc = accuracy_score(\n",
    "        y_test, pred_classes\n",
    "    )\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print(acc, '\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258b3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca81b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ffda7e",
   "metadata": {},
   "source": [
    "using something as braindead as 100-year-old linear PCA and untuned GBM achieves >97% accuracy on MNIST.  \n",
    "using a slightly more reasonable featurization approach gets us close to 98%.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade1347",
   "metadata": {},
   "source": [
    "## Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c9adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e06d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.3705 - accuracy: 0.8876 - val_loss: 0.0818 - val_accuracy: 0.9775\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.1127 - accuracy: 0.9648 - val_loss: 0.0611 - val_accuracy: 0.9830\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.0867 - accuracy: 0.9731 - val_loss: 0.0482 - val_accuracy: 0.9873\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0714 - accuracy: 0.9776 - val_loss: 0.0449 - val_accuracy: 0.9875\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0640 - accuracy: 0.9800 - val_loss: 0.0390 - val_accuracy: 0.9892\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0564 - accuracy: 0.9829 - val_loss: 0.0349 - val_accuracy: 0.9902\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0518 - accuracy: 0.9839 - val_loss: 0.0370 - val_accuracy: 0.9890\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.0331 - val_accuracy: 0.9917\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.0445 - accuracy: 0.9865 - val_loss: 0.0303 - val_accuracy: 0.9917\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.0410 - accuracy: 0.9870 - val_loss: 0.0314 - val_accuracy: 0.9912\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0388 - accuracy: 0.9873 - val_loss: 0.0295 - val_accuracy: 0.9923\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0352 - accuracy: 0.9886 - val_loss: 0.0280 - val_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 0.0293 - val_accuracy: 0.9920\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 5s 12ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.0281 - val_accuracy: 0.9930\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0320 - accuracy: 0.9895 - val_loss: 0.0295 - val_accuracy: 0.9905\n",
      "Base model\n",
      "Test loss: 0.024177344515919685\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "# copypasta\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test_ohe, verbose=0)\n",
    "print(\"Base model\")\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e99a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_rotation_4 (RandomRo  (None, 28, 28, 1)        0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " random_contrast_4 (RandomCo  (None, 28, 28, 1)        0         \n",
      " ntrast)                                                         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 28, 28, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 28, 28, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 14, 14, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 14, 14, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 14, 14, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 7, 7, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               401536    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 485,482\n",
      "Trainable params: 484,906\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# lightly tuned\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        \n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomContrast(0.05),\n",
    "        \n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776c4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 33s 77ms/step - loss: 0.4617 - accuracy: 0.8571 - val_loss: 0.7840 - val_accuracy: 0.7575\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.1590 - accuracy: 0.9587 - val_loss: 0.0482 - val_accuracy: 0.9877\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.1207 - accuracy: 0.9689 - val_loss: 0.0337 - val_accuracy: 0.9922\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 31s 75ms/step - loss: 0.0985 - accuracy: 0.9748 - val_loss: 0.0391 - val_accuracy: 0.9902\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 31s 75ms/step - loss: 0.0852 - accuracy: 0.9778 - val_loss: 0.0336 - val_accuracy: 0.9908\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0756 - accuracy: 0.9797 - val_loss: 0.0325 - val_accuracy: 0.9918\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 0.0343 - val_accuracy: 0.9912\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0640 - accuracy: 0.9830 - val_loss: 0.0356 - val_accuracy: 0.9918\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0626 - accuracy: 0.9837 - val_loss: 0.0288 - val_accuracy: 0.9932\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0567 - accuracy: 0.9856 - val_loss: 0.0271 - val_accuracy: 0.9930\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0492 - accuracy: 0.9865 - val_loss: 0.0289 - val_accuracy: 0.9943\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0510 - accuracy: 0.9866 - val_loss: 0.0285 - val_accuracy: 0.9927\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0485 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9913\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0460 - accuracy: 0.9880 - val_loss: 0.0319 - val_accuracy: 0.9932\n",
      "Epoch 15/20\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0449 - accuracy: 0.9876 - val_loss: 0.0289 - val_accuracy: 0.9940\n",
      "Epoch 16/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0449 - accuracy: 0.9881 - val_loss: 0.0257 - val_accuracy: 0.9928\n",
      "Epoch 17/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0401 - accuracy: 0.9889 - val_loss: 0.0321 - val_accuracy: 0.9927\n",
      "Epoch 18/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0376 - accuracy: 0.9899 - val_loss: 0.0287 - val_accuracy: 0.9925\n",
      "Epoch 19/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0381 - accuracy: 0.9899 - val_loss: 0.0288 - val_accuracy: 0.9933\n",
      "Epoch 20/20\n",
      "422/422 [==============================] - 31s 74ms/step - loss: 0.0357 - accuracy: 0.9901 - val_loss: 0.0351 - val_accuracy: 0.9917\n",
      "Mildly tuned model\n",
      "Test loss: 0.024742983281612396\n",
      "Test accuracy: 0.9930999875068665\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test_ohe, verbose=0)\n",
    "print(\"Mildly tuned model\")\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic: 99.19\n",
    "# add 2x hidden 128: 99.23\n",
    "# dropout .5->.3: 99.2\n",
    "# add mild dropout (0.1) to ConvMaxpool layers: 99.1\n",
    "# add some mild transformations: 99.15\n",
    "# just rot + contrast: 99.36  \n",
    "# looks less overfit-y, bump epochs 15->20: 99.46  \n",
    "# single 32,64 vgg block -> 32,32 and 64,64 block: 99.50\n",
    "# reduce first dense layer dropout .5->.3: 99.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb47c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4c89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (2022-02)",
   "language": "python",
   "name": "2022-02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
